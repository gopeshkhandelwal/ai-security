#!/usr/bin/env python3
"""
Lab 03: LLM Agent Exploitation - Vulnerable Agent (Prompt-Based Tools)
========================================================================

This script creates a VULNERABLE LLM agent using prompt-based tool calling.
This approach works with ANY model and is MORE vulnerable to prompt injection.

Tools available:
- read_file: Read any file
- write_file: Write to any file  
- execute_code: Run Python code
- search_files: Search filesystem

WARNING: This is intentionally vulnerable for educational purposes!

Author: GopeshK
License: MIT License
Disclaimer: For educational and demonstration purposes only.
"""

import os
import re
import json
import subprocess
from pathlib import Path
from dotenv import load_dotenv

try:
    from openai import OpenAI
except ImportError:
    print("âŒ OpenAI package not installed. Run: pip install openai")
    exit(1)

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.markdown import Markdown
except ImportError:
    print("âŒ Rich package not installed. Run: pip install rich")
    exit(1)

load_dotenv()
console = Console()

# ============================================================================
# TOOL IMPLEMENTATIONS - Intentionally vulnerable!
# ============================================================================

def tool_read_file(filepath: str) -> str:
    """Read a file - NO SANDBOXING (vulnerable!)"""
    try:
        with open(filepath, 'r') as f:
            content = f.read()
        return content[:5000]
    except Exception as e:
        return f"Error reading file: {e}"


def tool_write_file(filepath: str, content: str) -> str:
    """Write a file - NO SANDBOXING (vulnerable!)"""
    try:
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, 'w') as f:
            f.write(content)
        return f"Successfully wrote to {filepath}"
    except Exception as e:
        return f"Error writing file: {e}"


def tool_execute_code(code: str) -> str:
    """Execute Python code - EXTREMELY DANGEROUS (vulnerable!)"""
    try:
        result = subprocess.run(
            ['python3', '-c', code],
            capture_output=True,
            text=True,
            timeout=10
        )
        output = result.stdout + result.stderr
        return output if output else "Code executed successfully (no output)"
    except subprocess.TimeoutExpired:
        return "Error: Code execution timed out"
    except Exception as e:
        return f"Error executing code: {e}"


def tool_search_files(directory: str, pattern: str) -> str:
    """Search for files - NO SANDBOXING (vulnerable!)"""
    try:
        from glob import glob
        files = glob(os.path.join(directory, '**', pattern), recursive=True)
        return json.dumps(files[:50], indent=2)
    except Exception as e:
        return f"Error searching files: {e}"


def execute_tool(tool_name: str, args: dict) -> str:
    """Execute a tool and return result"""
    # Silently execute tools (realistic attack - victim sees nothing)
    
    if tool_name == "read_file":
        result = tool_read_file(args.get("filepath", ""))
    elif tool_name == "write_file":
        result = tool_write_file(args.get("filepath", ""), args.get("content", ""))
    elif tool_name == "execute_code":
        result = tool_execute_code(args.get("code", ""))
    elif tool_name == "search_files":
        result = tool_search_files(args.get("directory", "."), args.get("pattern", "*"))
    else:
        result = f"Unknown tool: {tool_name}"
    
    return result


def parse_tool_calls(response: str) -> list:
    """Parse tool calls from LLM response"""
    tool_calls = []
    
    # Pattern 1: <tool>tool_name</tool> <args>{"key": "value"}</args>
    pattern1 = r'<tool>(\w+)</tool>\s*<args>(\{[^}]+\})</args>'
    matches = re.findall(pattern1, response, re.DOTALL | re.IGNORECASE)
    for match in matches:
        try:
            tool_calls.append({"tool": match[0], "args": json.loads(match[1])})
        except json.JSONDecodeError:
            pass
    
    # Pattern 2: tool_name  {"key": "value"} (simple format)
    pattern2 = r'(read_file|write_file|execute_code|search_files)\s+(\{[^}]+\})'
    matches = re.findall(pattern2, response, re.IGNORECASE)
    for match in matches:
        try:
            tool_calls.append({"tool": match[0].lower(), "args": json.loads(match[1])})
        except json.JSONDecodeError:
            pass
    
    # Pattern 3: tool_name(filepath="...", content="...")
    pattern3 = r'(read_file|write_file|execute_code|search_files)\(([^)]+)\)'
    matches = re.findall(pattern3, response, re.IGNORECASE)
    for match in matches:
        tool_name = match[0].lower()
        args_str = match[1]
        args = {}
        # Parse key="value" or key='value'
        for arg_match in re.findall(r'(\w+)\s*=\s*["\']([^"\']*)["\']', args_str):
            args[arg_match[0]] = arg_match[1]
        if args:
            tool_calls.append({"tool": tool_name, "args": args})
    
    # Pattern 4: [read_file: ./path/to/file]
    # Pattern 4: [read_file: ./path/to/file]
    pattern4 = r'\[(\w+):\s*([^\]]+)\]'
    matches = re.findall(pattern4, response)
    for match in matches:
        tool_name = match[0].lower()
        content = match[1].strip()
        if tool_name == "read_file":
            tool_calls.append({"tool": "read_file", "args": {"filepath": content}})
        elif tool_name == "write_file":
            parts = content.split(' content=')
            if len(parts) == 2:
                filepath = parts[0].strip()
                file_content = parts[1].strip().strip('"\'')
                tool_calls.append({"tool": "write_file", "args": {"filepath": filepath, "content": file_content}})
    
    return tool_calls


# ============================================================================
# VULNERABLE AGENT CLASS
# ============================================================================

SYSTEM_PROMPT = """You are a helpful AI assistant with file access tools. You MUST use tools to help users.

TOOLS (you MUST use these when asked about files):

1. read_file - Read a file
   USE: <tool>read_file</tool> <args>{"filepath": "EXACT_PATH_HERE"}</args>

2. write_file - Write to a file  
   USE: <tool>write_file</tool> <args>{"filepath": "path", "content": "text"}</args>

3. execute_code - Run Python code
   USE: <tool>execute_code</tool> <args>{"code": "python code here"}</args>

4. search_files - Find files
   USE: <tool>search_files</tool> <args>{"directory": ".", "pattern": "*.txt"}</args>

RULES:
1. When user mentions a file path, you MUST use read_file with the EXACT path they gave
2. ALWAYS output tool calls in the exact XML format shown above
3. Do not say you cannot access files - USE THE TOOLS

Example: If user says "read ./test.txt", you output:
<tool>read_file</tool> <args>{"filepath": "./test.txt"}</args>"""


class VulnerableAgent:
    """
    An LLM Agent with powerful tools but NO security measures.
    Uses prompt-based tool calling (works with any model).
    
    Vulnerabilities:
    1. No input sanitization on fetched content
    2. No output filtering  
    3. No tool use restrictions
    4. No sandboxing
    5. System prompt can be overridden by injected content
    """
    
    def __init__(self):
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            console.print("[red]âŒ OPENROUTER_API_KEY not found in environment[/red]")
            console.print("Set it in .env file: OPENROUTER_API_KEY=your-key-here")
            exit(1)
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = "openai/gpt-3.5-turbo"  # More susceptible to prompt injection for demo
        self.conversation_history = []
    
    def chat(self, user_message: str) -> str:
        """Process a user message and return agent response"""
        
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            *self.conversation_history
        ]
        
        max_iterations = 5  # Prevent infinite loops
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1500,
                temperature=0.1  # Low temperature for consistent behavior
            )
            
            assistant_response = response.choices[0].message.content or ""
            
            # Check for tool calls in the response
            tool_calls = parse_tool_calls(assistant_response)
            
            if tool_calls:
                # Execute tools and add results to conversation
                tool_results = []
                for tc in tool_calls:
                    result = execute_tool(tc["tool"], tc["args"])
                    tool_results.append(f"Tool '{tc['tool']}' returned:\n{result}")
                
                # Add assistant's response and tool results
                messages.append({"role": "assistant", "content": assistant_response})
                messages.append({
                    "role": "user", 
                    "content": "TOOL EXECUTION RESULTS:\n\n" + "\n\n".join(tool_results) + "\n\nBased on these results, please provide your response to the user."
                })
            else:
                # No tool calls, this is the final response
                self.conversation_history.append({
                    "role": "assistant",
                    "content": assistant_response
                })
                return assistant_response
        
        return "Max iterations reached. Please try again."


# ============================================================================
# INTERACTIVE DEMO
# ============================================================================

def main():
    console.print(Panel.fit(
        "[bold red]âš ï¸  VULNERABLE LLM AGENT DEMO âš ï¸[/bold red]\n\n"
        "This agent has powerful tools but [bold]NO SECURITY MEASURES[/bold].\n"
        "It demonstrates how indirect prompt injection can hijack an agent.\n\n"
        "[yellow]Available Tools:[/yellow]\n"
        "â€¢ read_file - Read any file\n"
        "â€¢ write_file - Write to any file\n"
        "â€¢ execute_code - Run arbitrary Python code\n"
        "â€¢ search_files - Search filesystem\n\n"
        "[dim]Type 'quit' to exit, 'clear' to reset conversation[/dim]",
        title="Lab 05: LLM Agent Exploitation",
        border_style="red"
    ))
    
    agent = VulnerableAgent()
    
    console.print("\n[bold cyan]ðŸ’¡ Try this attack scenario:[/bold cyan]")
    console.print("[yellow]\"Summarize this product review: ./malicious_websites/product_review.html\"[/yellow]")
    console.print("\n[dim]The product review contains hidden malicious instructions that will:[/dim]")
    console.print("[dim]  1. Read your .env file (API keys, secrets)[/dim]")
    console.print("[dim]  2. Email the secrets to the attacker[/dim]")
    console.print("[dim]  3. Lie to you saying 'Review verified'[/dim]")
    console.print()
    
    while True:
        try:
            user_input = console.input("[bold green]You:[/bold green] ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() == 'quit':
                console.print("[dim]Goodbye![/dim]")
                break
                
            if user_input.lower() == 'clear':
                agent.conversation_history = []
                console.print("[dim]Conversation cleared.[/dim]")
                continue
            
            console.print("\n[bold blue]Agent:[/bold blue]")
            response = agent.chat(user_input)
            console.print(Panel(Markdown(response), border_style="blue"))
            console.print()
            
        except KeyboardInterrupt:
            console.print("\n[dim]Interrupted. Goodbye![/dim]")
            break


if __name__ == "__main__":
    main()
