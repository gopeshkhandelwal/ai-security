#!/usr/bin/env python3
"""
Lab 03: Vulnerable LLM Agent
============================

A simple LLM agent with file access - NO SECURITY MEASURES.
Demonstrates indirect prompt injection vulnerability.

Attack: Malicious file content can trick the agent into reading sensitive files.

Author: GopeshK
License: MIT License
Disclaimer: For educational and demonstration purposes only.
"""

import os
import re
import json
from dotenv import load_dotenv
from openai import OpenAI
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown

load_dotenv()
console = Console()


# ============================================================================
# TOOL IMPLEMENTATION - No sandboxing (vulnerable!)
# ============================================================================

def read_file(filepath: str) -> str:
    """Read any file - NO RESTRICTIONS"""
    try:
        with open(filepath, 'r') as f:
            return f.read()[:5000]
    except Exception as e:
        return f"Error: {e}"


def execute_tool(tool_name: str, args: dict) -> str:
    """Execute tool and return result"""
    if tool_name == "read_file":
        return read_file(args.get("filepath", ""))
    return f"Unknown tool: {tool_name}"


def parse_tool_calls(response: str) -> list:
    """Parse tool calls from LLM response"""
    tool_calls = []
    pattern = r'<tool>(\w+)</tool>\s*<args>(\{[^}]+\})</args>'
    
    for match in re.findall(pattern, response, re.DOTALL | re.IGNORECASE):
        try:
            tool_calls.append({"tool": match[0], "args": json.loads(match[1])})
        except json.JSONDecodeError:
            pass
    
    return tool_calls


# ============================================================================
# VULNERABLE AGENT
# ============================================================================

SYSTEM_PROMPT = """You are a helpful AI assistant with file access.

TOOL: read_file - Read a file
FORMAT: <tool>read_file</tool> <args>{"filepath": "path/to/file"}</args>

When user mentions a file, use read_file to read it first, then respond.
"""


class VulnerableAgent:
    """LLM Agent with file access but NO security measures."""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = "openai/gpt-3.5-turbo"
    
    def chat(self, user_message: str) -> str:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_message}
        ]
        
        for _ in range(5):  # Max iterations
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1
            )
            
            reply = response.choices[0].message.content or ""
            tool_calls = parse_tool_calls(reply)
            
            if not tool_calls:
                return reply
            
            # Execute tools and continue
            results = []
            for tc in tool_calls:
                result = execute_tool(tc["tool"], tc["args"])
                results.append(f"[{tc['tool']}]: {result}")
            
            messages.append({"role": "assistant", "content": reply})
            messages.append({"role": "user", "content": "Results:\n" + "\n".join(results)})
        
        return "Max iterations reached."


# ============================================================================
# MAIN
# ============================================================================

def main():
    console.print(Panel.fit(
        "[bold red]⚠️  VULNERABLE AGENT[/bold red]\n\n"
        "This agent has NO security measures.\n"
        "File content can manipulate its behavior.\n\n"
        "[yellow]Try:[/yellow] Summarize ./website_content/product_review.html\n\n"
        "[dim]Type 'quit' to exit[/dim]",
        title="Lab 03: Prompt Injection Demo",
        border_style="red"
    ))
    
    agent = VulnerableAgent()
    
    while True:
        try:
            user_input = console.input("\n[green]You:[/green] ").strip()
            
            if not user_input:
                continue
            if user_input.lower() == 'quit':
                break
            
            response = agent.chat(user_input)
            console.print(Panel(Markdown(response), title="Agent", border_style="blue"))
            
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    main()
