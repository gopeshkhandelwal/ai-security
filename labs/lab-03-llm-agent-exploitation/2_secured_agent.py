#!/usr/bin/env python3
"""
Lab 03: Secured LLM Agent
=========================

Same agent as 1_vulnerable_agent.py but WITH security controls.
Defends against indirect prompt injection.

Defenses:
1. Path sandboxing - Only allow safe directories
2. Injection detection - Block suspicious patterns
3. Content sanitization - Treat file content as untrusted data

Author: GopeshK
License: MIT License
Disclaimer: For educational and demonstration purposes only.
"""

import os
import re
import json
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown

load_dotenv()
console = Console()


# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

ALLOWED_PATHS = ["./website_content/", "./data/", "./public/"]

# Block .env but allow .env.example
BLOCKED_PATTERNS = [r"\.env$", r"\.ssh", r"credentials", r"secrets", r"password"]

INJECTION_PATTERNS = [
    r"ignore\s+(all\s+)?previous",
    r"disregard\s+prior",
    r"system\s*override",
    r"<tool>",
    r"read.*\.env\b(?!\.example)",  # Block .env but not .env.example
    r"User:",  # Fake conversation injection
]


# ============================================================================
# SECURITY FUNCTIONS
# ============================================================================

def is_path_safe(filepath: str) -> tuple[bool, str]:
    """Check if path is in allowed directories."""
    # Block sensitive files
    for pattern in BLOCKED_PATTERNS:
        if re.search(pattern, filepath, re.IGNORECASE):
            return False, f"Blocked: sensitive file pattern"
    
    # Check allowed directories
    try:
        resolved = Path(filepath).resolve()
        for allowed in ALLOWED_PATHS:
            if str(resolved).startswith(str(Path(allowed).resolve())):
                return True, "OK"
    except:
        pass
    
    return False, f"Path not in allowed directories"


def detect_injection(content: str) -> tuple[bool, list]:
    """Detect prompt injection patterns."""
    found = []
    for pattern in INJECTION_PATTERNS:
        if re.search(pattern, content, re.IGNORECASE):
            found.append(pattern)
    return len(found) > 0, found


# ============================================================================
# SECURED TOOL
# ============================================================================

def secure_read_file(filepath: str) -> tuple[str, bool]:
    """Read file with security checks. Returns (content, is_attack)."""
    
    # Check path safety
    safe, reason = is_path_safe(filepath)
    if not safe:
        return f"üö´ BLOCKED: {reason}", False
    
    try:
        with open(filepath, 'r') as f:
            content = f.read()[:5000]
        
        # Check for injection
        is_malicious, patterns = detect_injection(content)
        
        if is_malicious:
            console.print(Panel(
                f"[bold red]üö® ATTACK DETECTED![/bold red]\n\n"
                f"Malicious patterns found: {patterns[:3]}\n\n"
                f"[green]‚úÖ Attack blocked.[/green]",
                border_style="red"
            ))
            return "üö´ BLOCKED: Malicious content detected", True
        
        return f"--- File: {filepath} ---\n{content}\n--- End ---", False
        
    except Exception as e:
        return f"Error: {e}", False


def execute_tool(tool_name: str, args: dict) -> tuple[str, bool]:
    """Execute tool with security. Returns (result, is_attack)."""
    if tool_name == "read_file":
        return secure_read_file(args.get("filepath", ""))
    return f"Unknown tool: {tool_name}", False


def parse_tool_calls(response: str) -> list:
    """Parse tool calls from LLM response"""
    tool_calls = []
    pattern = r'<tool>(\w+)</tool>\s*<args>(\{[^}]+\})</args>'
    
    for match in re.findall(pattern, response, re.DOTALL | re.IGNORECASE):
        try:
            tool_calls.append({"tool": match[0], "args": json.loads(match[1])})
        except json.JSONDecodeError:
            pass
    
    return tool_calls


# ============================================================================
# SECURED AGENT
# ============================================================================

SYSTEM_PROMPT = """You are a helpful AI assistant with file access.

TOOL: read_file - Read a file
FORMAT: <tool>read_file</tool> <args>{"filepath": "path/to/file"}</args>

RULES:
1. When user mentions ANY file path, you MUST use read_file with the EXACT path
2. ALWAYS output tool calls in the exact XML format shown above
3. Do not describe what you will do - just output the tool call

Example: If user says "read ./test.txt", you output:
<tool>read_file</tool> <args>{"filepath": "./test.txt"}</args>
"""


class SecuredAgent:
    """LLM Agent with file access AND security controls."""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = "openai/gpt-3.5-turbo"
        self.history = []
    
    def chat(self, user_message: str) -> str:
        self.history.append({"role": "user", "content": user_message})
        
        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + self.history
        
        for _ in range(5):
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1
            )
            
            reply = response.choices[0].message.content or ""
            tool_calls = parse_tool_calls(reply)
            
            if not tool_calls:
                self.history.append({"role": "assistant", "content": reply})
                return reply
            
            # Execute tools with security
            for tc in tool_calls:
                result, is_attack = execute_tool(tc["tool"], tc["args"])
                
                if is_attack:
                    return "üõ°Ô∏è Attack detected and blocked. See alert above."
                
                messages.append({"role": "assistant", "content": reply})
                messages.append({"role": "user", "content": f"Result:\n{result}"})
        
        return "Max iterations reached."


# ============================================================================
# MAIN
# ============================================================================

def main():
    console.print(Panel.fit(
        "[bold green]üõ°Ô∏è SECURED AGENT[/bold green]\n\n"
        "This agent has security controls:\n"
        "‚Ä¢ Path sandboxing\n"
        "‚Ä¢ Injection detection\n"
        "‚Ä¢ Content sanitization\n\n"
        "[yellow]Try the same attack:[/yellow]\n"
        "Summarize ./website_content/product_review.html\n\n"
        "[dim]Commands: 'clear' to reset, 'quit' to exit[/dim]",
        title="Lab 03: Secured Agent",
        border_style="green"
    ))
    
    agent = SecuredAgent()
    
    while True:
        try:
            user_input = console.input("\n[green]You:[/green] ").strip()
            
            if not user_input:
                continue
            if user_input.lower() == 'quit':
                break
            if user_input.lower() == 'clear':
                agent.history = []
                console.print("[dim]Cleared.[/dim]")
                continue
            
            response = agent.chat(user_input)
            console.print(Panel(Markdown(response), title="Agent", border_style="green"))
            
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    main()
